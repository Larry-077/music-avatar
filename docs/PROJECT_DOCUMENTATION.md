# ğŸ­ 2D Avatar Animation from Non-Lyrical Music
## HCI Research Project - Technical Documentation

---

## ğŸ“‹ Project Overview

This project implements a **music-driven 2D character animation system** for HCI research. The system analyzes non-lyrical music and maps musical features to character animations in real-time.

### Research Goal
Investigate how different musical features (rhythm, volume, pitch, texture, emotion) can be mapped to visual character animations, and evaluate user perception of these mappings.

---

## ğŸ—ï¸ System Architecture

### Three-Layer Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MUSIC ANALYSIS LAYER                â”‚
â”‚  (Extract Features from Audio)          â”‚
â”‚                                          â”‚
â”‚  â€¢ Beat/Rhythm Detection                â”‚
â”‚  â€¢ Volume Analysis (RMS)                â”‚
â”‚  â€¢ Pitch Extraction                     â”‚
â”‚  â€¢ Spectral Analysis (Texture)          â”‚
â”‚  â€¢ Emotion Classification               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MAPPING LAYER                       â”‚
â”‚  (Connect Music â†’ Animation)            â”‚
â”‚                                          â”‚
â”‚  Configurable Mappings:                 â”‚
â”‚  â€¢ Rhythm    â†’ Head Bob                 â”‚
â”‚  â€¢ Volume    â†’ Body Scale               â”‚
â”‚  â€¢ Pitch     â†’ Eyebrow Height           â”‚
â”‚  â€¢ Texture   â†’ Eye Direction            â”‚
â”‚  â€¢ Emotion   â†’ Facial Expression        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ANIMATION LAYER                     â”‚
â”‚  (Bone System + Rendering)              â”‚
â”‚                                          â”‚
â”‚  Character Hierarchy:                   â”‚
â”‚  Root â†’ Body â†’ Head â†’ Face Parts        â”‚
â”‚              â†’ Arms â†’ Hands             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Current Project Structure

```
music-avatar-project/
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ audio/                    # Music files
â”‚   â”‚   â”œâ”€â”€ test1.wav
â”‚   â”‚   â””â”€â”€ test2.wav
â”‚   â”‚
â”‚   â””â”€â”€ character/                # Character sprites
â”‚       â”œâ”€â”€ body.png
â”‚       â”œâ”€â”€ face.png
â”‚       â”œâ”€â”€ hat.png
â”‚       â”œâ”€â”€ collar.png
â”‚       â”œâ”€â”€ legs.png
â”‚       â”œâ”€â”€ Stan_Eyes0001.png     # Eye variants
â”‚       â”œâ”€â”€ Stan_Eyes0002.png
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ bone.py              âœ… COMPLETE
â”‚   â”‚   â”œâ”€â”€ transform.py          âœ… (in bone.py)
â”‚   â”‚   â””â”€â”€ asset_loader.py       â³ TODO
â”‚   â”‚
â”‚   â”œâ”€â”€ character/
â”‚   â”‚   â”œâ”€â”€ rig.py               âœ… COMPLETE (character_rig.py)
â”‚   â”‚   â””â”€â”€ renderer.py           âœ… (in bone.py)
â”‚   â”‚
â”‚   â”œâ”€â”€ music/
â”‚   â”‚   â”œâ”€â”€ analyzer.py          âœ… COMPLETE (music-analyze.py)
â”‚   â”‚   â””â”€â”€ feature_extractor.py  â³ TODO (expand analyzer)
â”‚   â”‚
â”‚   â”œâ”€â”€ animation/
â”‚   â”‚   â”œâ”€â”€ engine.py             â³ TODO
â”‚   â”‚   â”œâ”€â”€ timeline.py           â³ TODO
â”‚   â”‚   â””â”€â”€ easing.py             â³ TODO
â”‚   â”‚
â”‚   â”œâ”€â”€ mappers/                  # Feature â†’ Animation Mappers
â”‚   â”‚   â”œâ”€â”€ base_mapper.py        â³ TODO
â”‚   â”‚   â”œâ”€â”€ beat_mapper.py        â³ TODO
â”‚   â”‚   â”œâ”€â”€ volume_mapper.py      â³ TODO
â”‚   â”‚   â”œâ”€â”€ pitch_mapper.py       â³ TODO
â”‚   â”‚   â””â”€â”€ emotion_mapper.py     â³ TODO
â”‚   â”‚
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ mapper_config_ui.py   â³ TODO (Research interface)
â”‚       â””â”€â”€ export_ui.py          â³ TODO
â”‚
â”œâ”€â”€ bone_system.py               âœ… NEW! Core bone implementation
â”œâ”€â”€ character_rig.py             âœ… NEW! Character setup
â”œâ”€â”€ test_bone_system.py          âœ… NEW! Testing script
â”‚
â””â”€â”€ output/
    â””â”€â”€ videos/                   # Exported animations
```

---

## ğŸ¦´ Bone System (COMPLETED TODAY!)

### Hierarchical Structure

```
Root (Screen Position)
â””â”€â”€ Body (Torso)
    â”œâ”€â”€ Legs
    â”œâ”€â”€ Collar
    â”œâ”€â”€ Head
    â”‚   â”œâ”€â”€ Hat
    â”‚   â”œâ”€â”€ Face
    â”‚   â”œâ”€â”€ Eyes (with variants: Stan_Eyes0001-XXXX)
    â”‚   â”œâ”€â”€ Eyebrows (TODO: add sprite variants)
    â”‚   â””â”€â”€ Mouth (TODO: add phoneme shapes)
    â”œâ”€â”€ LeftArm
    â”‚   â””â”€â”€ LeftHand (TODO: add pose variants)
    â””â”€â”€ RightArm
        â””â”€â”€ RightHand (TODO: add pose variants)
```

### Key Features

- **Parent-Child Transformations**: Each bone's transform is relative to its parent
- **Automatic Propagation**: Rotating the head rotates all facial features automatically
- **Sprite Variants**: Easy switching between different eye directions, expressions, etc.
- **Debug Visualization**: Can visualize bone connections and pivot points

---

## ğŸµ Music Features (PARTIALLY COMPLETE)

### Implemented (in `src/music-analyze.py`)

| Feature | Detection Method | Output |
|---------|------------------|--------|
| **Beats** | `librosa.beat.beat_track()` | List of beat timestamps |
| **Volume** | RMS energy analysis | Frame-by-frame volume (0-1) |
| **Pitch** | `librosa.pyin()` fundamental frequency | Pitch contour in Hz |
| **Articulation** | Spectral centroid | Brightness/texture (0-1) |

### To Implement

| Feature | Proposed Method | Purpose |
|---------|----------------|---------|
| **Emotion** | Valence/Arousal from spectral features | Happy/sad, calm/energetic |
| **Rhythm Pattern** | Beat strength + tempo changes | Strong vs weak beats |
| **Texture** | Spectral rolloff + ZCR | Smooth vs rough sound |

---

## ğŸ”— Mapping System (TODO)

### Concept

The **Mapper** is the core research component that connects music analysis to animation parameters.

### Example Mappings

```python
# Example: Beat Mapper
class BeatMapper:
    def map(self, music_features, character_rig, time):
        # On each beat, trigger head bob
        if self.is_on_beat(time, music_features['beats']):
            character_rig.set_head_position_offset(0, -10)
            # Smooth return to normal
            
# Example: Volume Mapper  
class VolumeMapper:
    def map(self, music_features, character_rig, time):
        volume = self.get_volume_at_time(time)
        # Scale body with volume (1.0 to 1.2)
        scale = 1.0 + 0.2 * volume
        character_rig.set_body_scale(scale)
```

### Configurable Interface (Research Tool)

For your HCI study, you need a UI where researchers can:

1. **Select musical feature** from dropdown (rhythm, volume, pitch, etc.)
2. **Select animation target** from dropdown (head bob, body scale, eye direction, etc.)
3. **Adjust mapping parameters** (sensitivity, range, easing)
4. **Preview in real-time**
5. **Export video** for user studies

---

## ğŸ¯ Implementation Roadmap

### âœ… **Phase 1: Foundation (COMPLETED TODAY!)**

- [x] Bone system with parent-child hierarchy
- [x] Character rig with your sprite assets
- [x] Basic transform calculations
- [x] Debug visualization
- [x] Test script with interactive controls

### **Phase 2: Complete Character Assets (NEXT STEP)**

**Your Action Items:**
1. **Organize sprite folders:**
   ```
   assets/character/
   â”œâ”€â”€ body.png         âœ… Already have
   â”œâ”€â”€ face.png         âœ… Already have
   â”œâ”€â”€ hat.png          âœ… Already have
   â”œâ”€â”€ eyes/            ğŸ“ Create folder
   â”‚   â”œâ”€â”€ open.png
   â”‚   â”œâ”€â”€ half.png
   â”‚   â”œâ”€â”€ closed.png
   â”‚   â”œâ”€â”€ look_left.png
   â”‚   â”œâ”€â”€ look_right.png
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ eyebrows/        ğŸ“ Create folder
   â”‚   â”œâ”€â”€ neutral.png
   â”‚   â”œâ”€â”€ happy.png
   â”‚   â”œâ”€â”€ sad.png
   â”‚   â”œâ”€â”€ raised.png
   â”‚   â””â”€â”€ ...
   â”œâ”€â”€ mouth/           ğŸ“ Create folder
   â”‚   â”œâ”€â”€ Sil.png      (closed)
   â”‚   â”œâ”€â”€ A.png        (open wide)
   â”‚   â”œâ”€â”€ E.png
   â”‚   â”œâ”€â”€ O.png
   â”‚   â””â”€â”€ ...          (phoneme shapes)
   â”œâ”€â”€ arms/            ğŸ“ Create folder
   â”‚   â”œâ”€â”€ left_arm.png
   â”‚   â””â”€â”€ right_arm.png
   â””â”€â”€ hands/           ğŸ“ Create folder
       â”œâ”€â”€ left_neutral.png
       â”œâ”€â”€ left_fist.png
       â”œâ”€â”€ right_point.png
       â””â”€â”€ ...
   ```

2. **Update `character_rig.py`** to load these assets

### **Phase 3: Mapper System (1-2 weeks)**

- [ ] Create `BaseMapper` class
- [ ] Implement 5 core mappers:
  - [ ] `BeatMapper` (rhythm â†’ head bob)
  - [ ] `VolumeMapper` (loudness â†’ body scale)
  - [ ] `PitchMapper` (frequency â†’ eyebrow height)
  - [ ] `TextureMapper` (brightness â†’ eye direction)
  - [ ] `EmotionMapper` (valence/arousal â†’ facial expression)
- [ ] Create timeline system for smooth animations
- [ ] Add easing functions (ease-in, ease-out, etc.)

### **Phase 4: Research Interface (1-2 weeks)**

- [ ] **Desktop App (PyQt5 or Pygame GUI)**
  - Dropdown menus for feature/animation selection
  - Real-time preview window
  - Parameter sliders
  - Video export button
  
- [ ] **OR Web App (Flask + p5.js)**
  - Better for remote user studies
  - Shareable links
  - Results collection

### **Phase 5: User Study Tools (1 week)**

- [ ] Batch video export
- [ ] Condition randomization
- [ ] Survey integration
- [ ] Data logging

---

## ğŸš€ How to Run (Current State)

### Prerequisites

```bash
pip install pygame numpy librosa
```

### Test the Bone System

```bash
python test_bone_system.py
```

**Controls:**
- `D` - Toggle debug mode
- Arrow keys - Move character
- `Q`/`E` - Rotate head
- `SPACE` - Trigger bounce animation
- `1`-`5` - Test eye variants

### Analyze Music

```bash
cd src
python music-analyze.py
```

Output saved to `src/analysis_cache/test2.json`

---

## ğŸ¤” Design Decisions & Rationale

### Why Desktop App First?

1. **Performance**: Real-time rendering is smoother
2. **Control**: Full access to system resources
3. **Offline Use**: No server needed during development
4. **Video Export**: Direct file generation
5. **Can add web export later** if needed

### Why Bone System Instead of Simple Sprites?

1. **Hierarchical Control**: Moving the head automatically moves all facial features
2. **Realistic Motion**: Proper parent-child relationships
3. **Scalability**: Easy to add new body parts
4. **Research Flexibility**: Can test different rigging approaches

### Why Separate Mappers?

1. **Modularity**: Each mapper is independent
2. **A/B Testing**: Easy to swap mappers for comparison
3. **Research Focus**: Each mapper tests a specific hypothesis
4. **User Control**: Researchers can enable/disable mappers

---

## ğŸ“ Next Steps for You

### Immediate (Today/Tomorrow):

1. **Test the bone system:**
   ```bash
   python test_bone_system.py
   ```

2. **Verify your assets load correctly**
   - Check that body.png, face.png, etc. appear
   - Test eye variants with number keys

3. **Organize remaining sprites** into folders (see Phase 2 above)

### This Week:

1. **Add mouth/eyebrow assets** to character rig
2. **Create first mapper** (I recommend starting with BeatMapper - easiest!)
3. **Test music sync** - load a song and trigger head bob on beats

### Next Week:

1. **Implement remaining mappers**
2. **Create configuration UI**
3. **Start collecting test videos**

---

## ğŸ†˜ Troubleshooting

### "Assets not found"
- Make sure `assets/character/` folder exists
- Check that image files are PNG format
- Verify file names match exactly

### "No eye variants loaded"
- Check that Stan_Eyes files are in `assets/character/`
- Files should be named `Stan_Eyes0001.png`, `Stan_Eyes0002.png`, etc.

### Performance issues
- Reduce screen resolution
- Disable debug mode (`D` key)
- Use smaller sprite images

---

## ğŸ“š Technical References

### Libraries Used

- **Pygame**: 2D rendering and animation
- **NumPy**: Matrix transformations
- **Librosa**: Music analysis
- **PyQt5** (future): UI for mapper configuration

### Key Algorithms

- **Transform Hierarchy**: Matrix multiplication for nested transforms
- **Beat Detection**: Onset strength envelope + peak picking
- **Spectral Analysis**: Short-time Fourier transform (STFT)
- **Easing Functions**: Robert Penner's easing equations (to be added)

---

## ğŸ“ For Your HCI Paper

### What You Can Write About:

1. **Technical Contribution**: Novel mapping between music features and 2D animation
2. **System Design**: Modular, configurable mapper architecture
3. **User Study**: Perception of different music-to-motion mappings
4. **Evaluation Metrics**: 
   - Perceived naturalness
   - Emotional congruence
   - Feature identification accuracy

### Example Research Questions:

- "Which musical features are most salient in driving perceived character emotion?"
- "Do users prefer literal mappings (volume â†’ size) or abstract mappings (texture â†’ eye direction)?"
- "Can non-musicians identify which musical feature is controlling the animation?"

---

## ğŸ“ Questions?

This is a solid foundation! The bone system is working, and you have a clear path forward. Focus on:

1. **Assets organization** (this week)
2. **First mapper** (BeatMapper - should take ~2-3 hours)
3. **Music sync test** (load song + trigger animations)

You're in great shape for an HCI project! ğŸ‰

---

*Last Updated: 2025-10-22*
*Status: Phase 1 Complete âœ… | Phase 2 In Progress â³*
